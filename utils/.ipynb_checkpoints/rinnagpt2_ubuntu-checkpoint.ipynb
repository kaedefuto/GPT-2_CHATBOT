{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KQOLR6Cfji"
   },
   "source": [
    "# **rinna版GPT-2のテキスト生成(Ubuntu)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUが使えるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1811,
     "status": "ok",
     "timestamp": 1619774299835,
     "user": {
      "displayName": "山本楓登",
      "photoUrl": "",
      "userId": "16058537755930222159"
     },
     "user_tz": -540
    },
    "id": "X7pgQ_wyRdCK",
    "outputId": "ff3cb571-0a8f-48c6-db41-76f330bce555"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71235,
     "status": "ok",
     "timestamp": 1619774385492,
     "user": {
      "displayName": "山本楓登",
      "photoUrl": "",
      "userId": "16058537755930222159"
     },
     "user_tz": -540
    },
    "id": "bMOUiANHulNR",
    "outputId": "54ebc852-7c9c-495d-b36f-0d94e4bc6f58",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.5.0\n",
      "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (0.10.2)\n",
      "Requirement already satisfied: sacremoses in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (0.0.45)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (4.49.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (4.0.1)\n",
      "Requirement already satisfied: packaging in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from transformers==4.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests->transformers==4.5.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests->transformers==4.5.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests->transformers==4.5.0) (2020.12.5)\n",
      "Requirement already satisfied: six in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
      "Requirement already satisfied: click in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.5.1\n",
      "    Uninstalling transformers-4.5.1:\n",
      "      Successfully uninstalled transformers-4.5.1\n",
      "Successfully installed transformers-4.5.0\n",
      "Requirement already satisfied: datasets==1.2.1 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (1.19.2)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (4.49.0)\n",
      "Requirement already satisfied: multiprocess in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (0.70.11.1)\n",
      "Requirement already satisfied: dill in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (0.3.3)\n",
      "Requirement already satisfied: xxhash in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (4.0.1)\n",
      "Requirement already satisfied: pandas in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (1.2.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from datasets==1.2.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from importlib-metadata->datasets==1.2.1) (3.4.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from pandas->datasets==1.2.1) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from pandas->datasets==1.2.1) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages (0.1.91)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.5.0\n",
    "!pip install datasets==1.2.1\n",
    "!pip install sentencepiece==0.1.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipadic\n",
      "  Using cached ipadic-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: ipadic\n",
      "Successfully installed ipadic-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ipadic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **run_clm.pyの変更**\n",
    "```\n",
    " ./transformers/examples/language-modeling/run_clm.py\n",
    " ```\n",
    "## 追加\n",
    "```\n",
    "from transformers import T5Tokenizer\n",
    "```\n",
    "## 変更\n",
    "```\n",
    "check_min_version(\"4.5.0\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfugsztIhfvc"
   },
   "source": [
    "## **ファインチューニングの実行**\n",
    "- work内にtrain.txtを置く\n",
    "- work内にval.txtを置く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開発用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132328,
     "status": "ok",
     "timestamp": 1619774574197,
     "user": {
      "displayName": "山本楓登",
      "photoUrl": "",
      "userId": "16058537755930222159"
     },
     "user_tz": -540
    },
    "id": "V45Xg_g6SmF4",
    "outputId": "f0e76721-af94-4661-a84b-bdfac4939300"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 事前学習の実行\n",
    "!python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
    "    --train_file=train.txt \\\n",
    "    --validation_file=val.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --save_steps=5000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --output_dir=output3/ \\\n",
    "    --use_fast_tokenizer=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/07/2021 11:03:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "09/07/2021 11:03:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=chat_new_5/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep07_11-03-56_DLBox-DS, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=10000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=chat_new_5/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset text/default-684cc426542fe76b (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/yamamoto/.cache/huggingface/datasets/text/default-684cc426542fe76b/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
      "Dataset text downloaded and prepared to /home/yamamoto/.cache/huggingface/datasets/text/default-684cc426542fe76b/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:490] 2021-09-07 11:03:59,248 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /home/yamamoto/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
      "[INFO|configuration_utils.py:526] 2021-09-07 11:03:59,249 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": 4096,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-09-07 11:04:02,740 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /home/yamamoto/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-09-07 11:04:02,740 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-09-07 11:04:02,741 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /home/yamamoto/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-09-07 11:04:02,741 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /home/yamamoto/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-09-07 11:04:02,741 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|modeling_utils.py:1052] 2021-09-07 11:04:03,492 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /home/yamamoto/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
      "[INFO|modeling_utils.py:1168] 2021-09-07 11:04:11,821 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1177] 2021-09-07 11:04:11,821 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/yamamoto/anaconda3/envs/gpt/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:175: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
      "100%|█████████████████████████████████████████| 133/133 [00:13<00:00, 10.15ba/s]\n",
      "100%|███████████████████████████████████████████| 49/49 [00:04<00:00, 10.36ba/s]\n",
      "./transformers/examples/pytorch/language-modeling/run_clm.py:335: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
      "09/07/2021 11:04:29 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
      "100%|█████████████████████████████████████████| 133/133 [00:15<00:00,  8.86ba/s]\n",
      "100%|███████████████████████████████████████████| 49/49 [00:05<00:00,  9.02ba/s]\n",
      "[INFO|trainer.py:1013] 2021-09-07 11:04:52,146 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-09-07 11:04:52,147 >>   Num examples = 4346\n",
      "[INFO|trainer.py:1015] 2021-09-07 11:04:52,147 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1016] 2021-09-07 11:04:52,147 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1017] 2021-09-07 11:04:52,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1018] 2021-09-07 11:04:52,147 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1019] 2021-09-07 11:04:52,147 >>   Total optimization steps = 13038\n",
      "{'loss': 3.9076, 'learning_rate': 4.808252799509128e-05, 'epoch': 0.12}         \n",
      "{'loss': 3.6869, 'learning_rate': 4.6165055990182545e-05, 'epoch': 0.23}        \n",
      "{'loss': 3.6372, 'learning_rate': 4.424758398527381e-05, 'epoch': 0.35}         \n",
      "{'loss': 3.612, 'learning_rate': 4.233011198036509e-05, 'epoch': 0.46}          \n",
      "{'loss': 3.5812, 'learning_rate': 4.041263997545636e-05, 'epoch': 0.58}         \n",
      "{'loss': 3.5516, 'learning_rate': 3.849516797054763e-05, 'epoch': 0.69}         \n",
      "{'loss': 3.5391, 'learning_rate': 3.6577695965638905e-05, 'epoch': 0.81}        \n",
      "{'loss': 3.5271, 'learning_rate': 3.466022396073017e-05, 'epoch': 0.92}         \n",
      "{'loss': 3.4532, 'learning_rate': 3.274275195582145e-05, 'epoch': 1.04}         \n",
      "{'loss': 3.2922, 'learning_rate': 3.0825279950912716e-05, 'epoch': 1.15}        \n",
      "{'loss': 3.3032, 'learning_rate': 2.890780794600399e-05, 'epoch': 1.27}         \n",
      "{'loss': 3.3062, 'learning_rate': 2.6990335941095262e-05, 'epoch': 1.38}        \n",
      "{'loss': 3.319, 'learning_rate': 2.507286393618653e-05, 'epoch': 1.5}           \n",
      "{'loss': 3.2838, 'learning_rate': 2.3155391931277805e-05, 'epoch': 1.61}        \n",
      "{'loss': 3.2729, 'learning_rate': 2.1237919926369076e-05, 'epoch': 1.73}        \n",
      "{'loss': 3.2908, 'learning_rate': 1.9320447921460347e-05, 'epoch': 1.84}        \n",
      "{'loss': 3.2716, 'learning_rate': 1.740297591655162e-05, 'epoch': 1.96}         \n",
      "{'loss': 3.1832, 'learning_rate': 1.548550391164289e-05, 'epoch': 2.07}         \n",
      "{'loss': 3.13, 'learning_rate': 1.3568031906734163e-05, 'epoch': 2.19}          \n",
      "{'loss': 3.1337, 'learning_rate': 1.1650559901825433e-05, 'epoch': 2.3}         \n",
      " 77%|██████████████████████████▊        | 10000/13038 [1:06:41<20:20,  2.49it/s][INFO|trainer.py:1648] 2021-09-07 12:11:33,652 >> Saving model checkpoint to chat_new_5/checkpoint-10000\n",
      "[INFO|configuration_utils.py:329] 2021-09-07 12:11:33,652 >> Configuration saved in chat_new_5/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-09-07 12:11:34,877 >> Model weights saved in chat_new_5/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1901] 2021-09-07 12:11:34,877 >> tokenizer config file saved in chat_new_5/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-09-07 12:11:34,877 >> Special tokens file saved in chat_new_5/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|tokenization_t5.py:287] 2021-09-07 12:11:34,878 >> Copy vocab file to chat_new_5/checkpoint-10000/spiece.model\n",
      "{'loss': 3.1165, 'learning_rate': 9.733087896916706e-06, 'epoch': 2.42}         \n",
      "{'loss': 3.1283, 'learning_rate': 7.815615892007977e-06, 'epoch': 2.53}         \n",
      "{'loss': 3.1381, 'learning_rate': 5.8981438870992485e-06, 'epoch': 2.65}        \n",
      "{'loss': 3.1443, 'learning_rate': 3.980671882190521e-06, 'epoch': 2.76}         \n",
      "{'loss': 3.1399, 'learning_rate': 2.0631998772817916e-06, 'epoch': 2.88}        \n",
      "{'loss': 3.1344, 'learning_rate': 1.4572787237306336e-07, 'epoch': 2.99}        \n",
      "100%|███████████████████████████████████| 13038/13038 [1:27:03<00:00,  2.49it/s][INFO|trainer.py:1196] 2021-09-07 12:31:56,050 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5223.9033, 'train_samples_per_second': 2.496, 'epoch': 3.0}   \n",
      "100%|███████████████████████████████████| 13038/13038 [1:27:03<00:00,  2.50it/s]\n",
      "[INFO|trainer.py:1648] 2021-09-07 12:31:56,051 >> Saving model checkpoint to chat_new_5/\n",
      "[INFO|configuration_utils.py:329] 2021-09-07 12:31:56,051 >> Configuration saved in chat_new_5/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-09-07 12:31:57,265 >> Model weights saved in chat_new_5/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1901] 2021-09-07 12:31:57,266 >> tokenizer config file saved in chat_new_5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-09-07 12:31:57,266 >> Special tokens file saved in chat_new_5/special_tokens_map.json\n",
      "[INFO|tokenization_t5.py:287] 2021-09-07 12:31:57,266 >> Copy vocab file to chat_new_5/spiece.model\n",
      "[INFO|trainer_pt_utils.py:722] 2021-09-07 12:31:57,267 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:31:57,267 >>   epoch                    =        3.0\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:31:57,267 >>   train_runtime            = 1:27:03.90\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:31:57,267 >>   train_samples            =       4346\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:31:57,267 >>   train_samples_per_second =      2.496\n",
      "09/07/2021 12:31:57 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1865] 2021-09-07 12:31:57,268 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1866] 2021-09-07 12:31:57,268 >>   Num examples = 1583\n",
      "[INFO|trainer.py:1867] 2021-09-07 12:31:57,268 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████| 1583/1583 [02:55<00:00,  9.00it/s]\n",
      "[INFO|trainer_pt_utils.py:722] 2021-09-07 12:34:53,119 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   epoch                   =        3.0\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   eval_loss               =     3.5013\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   eval_runtime            = 0:02:55.85\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   eval_samples            =       1583\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   eval_samples_per_second =      9.002\n",
      "[INFO|trainer_pt_utils.py:727] 2021-09-07 12:34:53,119 >>   perplexity              =     33.157\n",
      "CPU times: user 1min 37s, sys: 26.8 s, total: 2min 4s\n",
      "Wall time: 1h 30min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 事前学習の実行\n",
    "!python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
    "    --train_file=train_new.txt \\\n",
    "    --validation_file=train2.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_train_epochs=3 \\\n",
    "    --save_steps=10000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --output_dir=chat_new_5/ \\\n",
    "    --use_fast_tokenizer=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModelForCausal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おはよう\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_9/yvs2pc_53n1cx_81482_bh5h0000gn/T/ipykernel_2023/3764468806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m             ]\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mte\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_single_text_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"おはよう\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_9/yvs2pc_53n1cx_81482_bh5h0000gn/T/ipykernel_2023/3764468806.py\u001b[0m in \u001b[0;36mcreate_single_text_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# トークナイザーとモデルの準備\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rinna/japanese-gpt2-medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print(tokenizer.all_special_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chat/lib/python3.7/site-packages/transformers/utils/dummy_sentencepiece_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mrequires_sentencepiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chat/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_sentencepiece\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCEPIECE_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#import torch.cuda.amp as amp\n",
    "from transformers import T5Tokenizer, AutoModelForCausalLM, TFAutoModelForCausalLM, AutoTokenizer\n",
    "import csv\n",
    "\n",
    "\n",
    "from transformers import T5Tokenizer, AutoModelForCausalLM, TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def create_single_text_message(message):\n",
    "    \"\"\"\n",
    "    if message == 'ありがとう':\n",
    "        message = 'どういたしまして！'\n",
    "    if message == '今日の天気は':\n",
    "        message = '雨に決まってるだろうが'\n",
    "    if message == 'こんにちは':\n",
    "        message = 'こんちはー'\n",
    "    \"\"\"\n",
    "    #sentence=message\n",
    "    print(message)\n",
    "    #\"\"\"\n",
    "    #入力文字\n",
    "    enter = '<s>{}[SEP]'.format(message)\n",
    "\n",
    "    # トークナイザーとモデルの準備\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "    #print(tokenizer.all_special_ids)\n",
    "\n",
    "    #モデル\n",
    "    #model = AutoModelForCausalLM.from_pretrained(\"../../chat_new_5\")\n",
    "    #model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"kaedefuto/chat_bot\")\n",
    "    # 推論\n",
    "    pt_tensor = tokenizer.encode(enter, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=pt_tensor,\n",
    "        do_sample=True,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        early_stopping=True,\n",
    "        max_length=50,\n",
    "        min_length=10,\n",
    "        num_return_sequences=1,\n",
    "        output_scores=True,\n",
    "        bad_words_ids=[[1], [5]]\n",
    "    )\n",
    "    outer = tokenizer.batch_decode(output)\n",
    "\n",
    "    #</s>の削除\n",
    "    outer = [s.replace('</s>','') for s in outer]\n",
    "    outer = [s.replace('<unk>','') for s in outer]\n",
    "    for i in outer:\n",
    "        sentence=i.replace(\"<s>\",\"\").replace(\"[SEP]\",\"\").replace(message,\"\")\n",
    "\n",
    "    print(sentence)\n",
    "    #\"\"\"\n",
    "    test_message = [\n",
    "                {\n",
    "                    'type': 'text',\n",
    "                    'text': sentence\n",
    "                }\n",
    "            ]\n",
    "    return test_message\n",
    "te=create_single_text_message(\"おはよう\")\n",
    "print(te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TFAutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from transformers import T5Tokenizer, AutoModelForCausalLM, TFAutoModelForCausalLM\n",
    "import csv\n",
    "\n",
    "#入力文字\n",
    "enter = ['ソファの種類はカウチなので、', 'ソファの種類はコーナーなので、', 'ソファの種類はリクライニングなので、', 'ソファの種類はフロアソファなので、', 'カウチタイプのソファとなっているので、', 'コーナーソファとなっているので、', 'リクライニングソファとなっているので、', 'フロアソファとなっているので、']\n",
    "name = 'e5_type'\n",
    "num = 1 \n",
    "\n",
    "header = [\"入力\",\"出力\"]\n",
    "with open(\"./csv/{0}.csv\".format(name), 'w', encoding=\"sjis\") as f:\n",
    "  writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "  writer.writerow(header)\n",
    "\n",
    "# トークナイザーとモデルの準備\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "tf_model = TFAutoModelForCausalLM.from_pretrained(\"output_e5/\")\n",
    "\n",
    "with amp.autocast():\n",
    "    for sentence in enter:\n",
    "        print(num)\n",
    "      # 推論\n",
    "        tf_tensor = tokenizer.encode(sentence, return_tensors=\"tf\")\n",
    "        output = tf_model.generate(\n",
    "            input_ids=pt_tensor,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            early_stopping=True,\n",
    "            max_length=50,\n",
    "            min_length=20,\n",
    "            num_return_sequences=25,\n",
    "            output_scores=True\n",
    "        )\n",
    "        outer = tokenizer.batch_decode(output)\n",
    "\n",
    "        #</s>の削除\n",
    "        outer = [s.replace('</s> ','') for s in outer]\n",
    "        outer = [s.replace('</s>','') for s in outer]\n",
    "        length = len(outer)\n",
    "        print(outer)\n",
    "    #   for i in range(length):\n",
    "    #     outer[i] = outer[i].replace('</s>','')\n",
    "    #   print(outer)\n",
    "\n",
    "        #CSV書き込み\n",
    "#         with open(\"./csv/{0}.csv\".format(name), 'a', encoding=\"sjis\") as f:\n",
    "#             writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "#             one_row = [sentence, '']\n",
    "#             writer.writerow(one_row)\n",
    "#             for i in range(length):\n",
    "#                 one_row = [\"\", outer[i]]\n",
    "#                 writer.writerow(one_row)\n",
    "        num += 1\n",
    "\n",
    "print('complete')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rinnagpt2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
